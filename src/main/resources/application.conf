server {
    host="0.0.0.0"
    port=8080
}

llm {
    endpoint="https://api.openai.com/v1/chat/completions"
}